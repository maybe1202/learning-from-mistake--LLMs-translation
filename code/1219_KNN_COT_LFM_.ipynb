{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSwaAGShdKjg"
      },
      "source": [
        "# 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYbh4tcWmDUH",
        "outputId": "d69c390e-eceb-4bc4-d7a5-396bf9698e67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3Dx6zu9L7aq"
      },
      "outputs": [],
      "source": [
        "# Datastore的資料位置，測試資料在 Main 裡面\n",
        "path = {\n",
        "    \"chinese_datastore\": \"馬蘭阿美族語 那些詞的整句(包括翻譯).json\", # 那些詞的整句(包括翻譯).json\n",
        "    \"words_datastore\": \"new馬蘭阿美語  每個詞的翻譯.json\" # 每個詞的翻譯.json\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz3ebbo1jIRl"
      },
      "source": [
        "# 1. Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aN52XCHjqv5"
      },
      "source": [
        "## i. Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8zUK6TddOB4"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXytTMqQdhR1",
        "outputId": "da15ea33-6aca-49b0-b026-7e5788c80e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Collecting torch\n",
            "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m917.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, openai, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.1\n",
            "    Uninstalling transformers-4.38.1:\n",
            "      Successfully uninstalled transformers-4.38.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed faiss-cpu-1.8.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 openai-0.28.1 sentence-transformers-2.5.1 torch-2.2.1 transformers-4.38.2 triton-2.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -U torch openai==0.28.1 nltk transformers faiss-cpu sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGkwSEkOdWp-"
      },
      "source": [
        "### Datastore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB3L3ojTLH7l"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "\n",
        "all_ch2amis = {value: key for key, value in json.load(open(path[\"chinese_datastore\"], \"r\")).items()}\n",
        "words = list(json.load(open(path[\"words_datastore\"], \"r\")).items())\n",
        "word_list = list((w[1] for w in words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bNaDPZdpu4"
      },
      "source": [
        "## ii. Find knn examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OYt_b9us0yI"
      },
      "source": [
        "### Setup BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpG3n56IL7hu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the BERT model\n",
        "model_name = 'DMetaSoul/sbert-chinese-general-v2'\n",
        "#model_name = 'bert-base-chinese'\n",
        "tokenizer = AutoTokenizer.from_pretrained('DMetaSoul/sbert-chinese-general-v2')\n",
        "model = AutoModel.from_pretrained('DMetaSoul/sbert-chinese-general-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJoT7VGYNCmD"
      },
      "source": [
        "### 建立faiss資料集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVMXq86Q-AR9"
      },
      "outputs": [],
      "source": [
        "import jieba\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01us4VaFxz-9"
      },
      "outputs": [],
      "source": [
        "# 把句子轉換為嵌入向量\n",
        "def get_single_embedding(sentence): # input type: string\n",
        "  output = model(**tokenizer(sentence, return_tensors='pt', truncation=True, padding=True))\n",
        "  embedding = output.last_hidden_state.mean(dim=1).detach().numpy()[0]\n",
        "  return embedding\n",
        "\n",
        "# 批量轉換\n",
        "def get_multiple_embeddings(sentences): # input type: list\n",
        "  embeddings = {}\n",
        "  for sentence in sentences:\n",
        "    embeddings[sentence] = get_single_embedding(sentence)\n",
        "  return embeddings # return type: dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNoCQAa26qDV"
      },
      "outputs": [],
      "source": [
        "# 將嵌入向量的list轉換為Faiss索引\n",
        "def embeddings2faiss(embeddings): # input type: list\n",
        "\n",
        "  # 將嵌入向量轉換為Numpy數組\n",
        "  np_embeddings = np.vstack(embeddings)\n",
        "\n",
        "  #獲取嵌入維度並創建faiss索引\n",
        "  faiss_index = faiss.IndexFlatL2(len(np_embeddings[0]))\n",
        "\n",
        "  # 将数据添加到Faiss索引\n",
        "  faiss_index.add(np_embeddings)\n",
        "\n",
        "  return faiss_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZJjujq9_VZY"
      },
      "outputs": [],
      "source": [
        "#建立datastore的embeddings data\n",
        "#這裡還不轉換為faiss index是因為後續[抽資料]跟[錯誤反思]會需要從datastore中刪資料，faiss index為list形式，較難於之後處裡\n",
        "#embeddings2faiss速度很快不用怕\n",
        "all_datastore_embeddings = get_multiple_embeddings(list(all_ch2amis.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhA6dRknAj_R"
      },
      "outputs": [],
      "source": [
        "# 建立words的faiss index data\n",
        "word_embeddings = []\n",
        "for word in word_list:\n",
        "  word_embeddings.append(get_single_embedding(word))\n",
        "index_word = embeddings2faiss(word_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmuH9X1gNLy3"
      },
      "source": [
        "### Knn functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lujE0ooeQKt5"
      },
      "outputs": [],
      "source": [
        "# 把中文的攤開來\n",
        "word_dic = {} # {1 vec index: 2 vec index}\n",
        "# one_vec_word_list = []\n",
        "index_cnt = 0\n",
        "for e1, w1 in enumerate(word_list):\n",
        "  for e2, w2 in enumerate(w1):\n",
        "    # one_vec_word_list.append(w2)\n",
        "    word_dic[index_cnt] = [e1, e2] # e1: 原本list的第幾個, e2: list內的第幾個詞語\n",
        "    index_cnt+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VanqVFYr9LH0"
      },
      "outputs": [],
      "source": [
        "def find_similar(find_word):\n",
        "\n",
        "    find_word_embeddings = get_single_embedding(find_word)\n",
        "\n",
        "    distances, indices = index_word.search(np.array([find_word_embeddings]), k=1)\n",
        "\n",
        "    matching_key = next(key for key, value in word_dic.items() if value[0] == indices[0])\n",
        "\n",
        "    return  word_dic[int(matching_key)] # index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRUFomg19hoI"
      },
      "outputs": [],
      "source": [
        "# 尋找最長相同子字串\n",
        "def find_longest_same_voc(sentance):\n",
        "    temp_list = []\n",
        "    temp_len_list = []\n",
        "    ans_list = []\n",
        "    for e, w in enumerate(word_list):\n",
        "        for e2,  w2 in enumerate(w):\n",
        "            if len(sentance) >= len(w2):\n",
        "                if w2 == sentance[0:len(w2)]:\n",
        "                    # print(\"len(w2):\"+str(len(w2)))\n",
        "                    temp_list.append([e, e2])\n",
        "                    # print(\"len(w2):\"+str([e, e2]))\n",
        "                    temp_len_list.append(len(w2))\n",
        "    if len(temp_len_list) != 0:\n",
        "        max_len = max(temp_len_list)\n",
        "        # print(\"max_len：\"+ str(max_len))\n",
        "        for e, i in enumerate(temp_len_list):\n",
        "            if i == max_len:\n",
        "                ans_list.append(temp_list[e])\n",
        "\n",
        "    return ans_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ega5hQ579lHr"
      },
      "outputs": [],
      "source": [
        "def check_not_found_sentance(sentence):\n",
        "    # print(\"check_not_found_sentance:\", sentence)\n",
        "    if len(sentence) == 0:\n",
        "        return \"\"\n",
        "    ans = \"\"\n",
        "    seg_list = jieba.cut(sentence, cut_all=False)\n",
        "    for text in list(seg_list):\n",
        "      if text != ' ':\n",
        "        e = find_similar(text)\n",
        "        ans += \"[*zh]: \" + text + \"\\n\" + \"[zh]: \" + word_list[e[0]][e[1]] + \"\\n\" + \"[amis]: \" + words[e[0]][0] + \"\\n\\n\"\n",
        "\n",
        "        # ans += \"[*zh]: \" + text + \"[zh]: \" + word_list[e[0]][e[1]] +  \", \" + \"[amis]: \" + words[e[0]][0]\n",
        "    return ans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tj-AxSl9nT5"
      },
      "outputs": [],
      "source": [
        "# 主func\n",
        "def trans(sentance):\n",
        "    cant_find_sentance = \"\"\n",
        "    ans = \"\"\n",
        "    while len(sentance) != 0:\n",
        "        index_list = find_longest_same_voc(sentance)\n",
        "        if len(index_list) != 0:\n",
        "            ans += check_not_found_sentance(cant_find_sentance)\n",
        "            cant_find_sentance = \"\"\n",
        "            # print(index_list)\n",
        "\n",
        "            if index_list[0][1] == -1:\n",
        "                sentance = sentance[len(word_list[index_list[0][0]]):]\n",
        "            else:\n",
        "                sentance = sentance[len(word_list[index_list[0][0]][index_list[0][1]]):]\n",
        "            for index in index_list:\n",
        "\n",
        "                if index[1] == -1:\n",
        "                    ans += \"[zh]: \" + word_list[index[0]] + \"\\n\" + \"[amis]: \" + words[index[0]][0] + \"\\n\\n\"\n",
        "                else:\n",
        "                    ans += \"[zh]: \" + word_list[index[0]][index[1]] + \"\\n\" + \"[amis]: \" + words[index[0]][0] + \"\\n\\n\"\n",
        "                # if index[1] == -1:\n",
        "                #     ans += \"[zh]: \" + word_list[index[0]] + \"[amis]: \" + words[index[0]][0]\n",
        "                # else:\n",
        "                #     ans += \"[zh]: \" + word_list[index[0]][index[1]] + \"[amis]: \" + words[index[0]][0]\n",
        "\n",
        "        else:\n",
        "            cant_find_sentance += sentance[0]\n",
        "            sentance = sentance[1:]\n",
        "    ans += check_not_found_sentance(cant_find_sentance)\n",
        "    return ans\n",
        "# print(trans(\"野地裡有花\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEPlC9eANm96"
      },
      "outputs": [],
      "source": [
        "def find_knn_examples_topN_sentence(sentence, datastore_embeddings, k):\n",
        "\n",
        "  cp_datastore_embeddings = dict(datastore_embeddings)\n",
        "  if sentence in cp_datastore_embeddings.keys():\n",
        "    del cp_datastore_embeddings[sentence]\n",
        "\n",
        "  datastore_index = embeddings2faiss(list(cp_datastore_embeddings.values()))\n",
        "\n",
        "  examples = []\n",
        "  distances, indices = datastore_index.search(np.array([get_single_embedding(sentence)]), k)\n",
        "\n",
        "  for i, index in enumerate(indices[0]):\n",
        "    zh_example = list(cp_datastore_embeddings.keys())[index]\n",
        "    examples.append([zh_example, all_ch2amis[zh_example]])\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwS8ijj54eNx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def find_knn_examples(sentence, datastore_embeddings, k, findwords=True):\n",
        "\n",
        "  examples = \"\"\n",
        "  for i in find_knn_examples_topN_sentence(sentence, datastore_embeddings, k):\n",
        "    examples += \"[zh]:\" + i[0] + \"\\n\"\n",
        "    examples += \"[amis]:\" + i[1] + \"\\n\\n\"\n",
        "\n",
        "\n",
        "  if findwords:\n",
        "    # 去除標點符號\n",
        "    translator = str.maketrans(\"，。！\", \"   \", string.punctuation)\n",
        "    tr_sentence = sentence.translate(translator)\n",
        "\n",
        "    # 去除英文字\n",
        "    sentence_without_english = re.sub(r'[a-zA-Z]+', '', tr_sentence)\n",
        "\n",
        "    # 查中文詞表\n",
        "    examples += trans(sentence_without_english)\n",
        "\n",
        "    # 英文照翻不變\n",
        "    get_english = re.findall(r'[a-zA-Z]+', tr_sentence)\n",
        "    for word in get_english:\n",
        "      examples += \"[zh]:\"+word+\"\\n\"\n",
        "      examples += \"[amis]:\"+word+\"\\n\\n\"\n",
        "\n",
        "  # print(examples)\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SliAMNwxeOrT"
      },
      "source": [
        "## iii. Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbtpLl50mFoZ"
      },
      "source": [
        "### Openai prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgEvKCZ9KhnT",
        "outputId": "fb54b333-088a-46c8-f8bb-d9a79f070932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement request (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for request\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d8jABamfvBe"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import json\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# url = \"\"\n",
        "# #tokenizer = AutoTokenizer.from_pretrained(\"MediaTek-Research/Breeze-7B-Instruct-v0.1\")\n",
        "\n",
        "# def Breeze_st(messages):\n",
        "#     prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "#     payload = json.dumps({\n",
        "#         \"inputs\": prompt,\n",
        "#         \"parameters\": {\n",
        "#             \"do_sample\": True,\n",
        "#             \"temperature\": 0.01,\n",
        "#             \"top_p\": 0.95\n",
        "#         }\n",
        "#     })\n",
        "#     headers = {\n",
        "#         'Content-Type': 'application/json',\n",
        "#         'accept': 'application/json'\n",
        "#     }\n",
        "\n",
        "#     response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "#     return response.text\n",
        "\n",
        "# def translate_ch2amis(sentence, datastore_embeddings, knn_k=10, cot_num=2, findwords=True):\n",
        "#     examples = find_knn_examples(sentence, datastore_embeddings, knn_k, findwords)\n",
        "#     cot_examples = find_knn_examples_topN_sentence(sentence, datastore_embeddings, cot_num)\n",
        "\n",
        "#     messages = []\n",
        "#     for i in cot_examples:\n",
        "#         messages.append({\"role\": \"user\", \"content\":\"You are an amis language translator. The followings some [zh] to [amis] examples. \\n\" + find_knn_examples(i[0], datastore_embeddings, knn_k, findwords) + \"If you see [*zh], it means that I couldn't find it in my dictionary, so I have identified the term closest in meaning. Please help me determine whether it has the same meaning as the next [zh] (the one I identified as the closest). If it does, please refer to it , If they are not similar, ignore it. If you see English, it means a proper noun, Please ignore what it brings back , and please put it directly into the translated sentence when translating, don't ignore it. cloud you help to translate[zh]:\" + i[0]})\n",
        "#         messages.append({\"role\": \"assistant\", \"content\": i[1]})\n",
        "\n",
        "#     messages.append({\"role\": \"user\", \"content\":\"You are an amis language translator. The followings some [zh] to [amis] examples. \\n\" + examples + \"If you see [*zh], it means that I couldn't find it in my dictionary, so I have identified the term closest in meaning. Please help me determine whether it has the same meaning as the next [zh] (the one I identified as the closest). If it does, please refer to it , If they are not similar, ignore it. cloud you help to translate[zh]:\" + sentence})\n",
        "\n",
        "#     return Breeze_st(messages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PUZd2LijKw0w",
        "outputId": "f1f91474-0165-46dc-cb0a-ff8ebaddea7a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<s>You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.   [INST] 你好，請問你可以完成什麼任務？ [/INST] 你好，我可以幫助您解決各種問題、提供資訊和協助您完成許多不同的任務。例如：回答技術問題、提供建議、翻譯文字、尋找資料或協助您安排行程等。請告訴我如何能幫助您。 [INST] 太棒了！ [/INST] '"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from transformers import AutoTokenizer\n",
        "# #tokenizer = AutoTokenizer.from_pretrained(\"MediaTek-Research/Breeze-7B-Instruct-v0.1\")\n",
        "# chat = [\n",
        "#   {\"role\": \"user\", \"content\": \"你好，請問你可以完成什麼任務？\"},\n",
        "#   {\"role\": \"assistant\", \"content\": \"你好，我可以幫助您解決各種問題、提供資訊和協助您完成許多不同的任務。例如：回答技術問題、提供建議、翻譯文字、尋找資料或協助您安排行程等。請告訴我如何能幫助您。\"},\n",
        "#   {\"role\": \"user\", \"content\": \"太棒了！\"},\n",
        "# ]\n",
        "# tokenizer.apply_chat_template(chat, tokenize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztqneJ2sN7u-"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import json\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# url = \"\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"MediaTek-Research/Breeze-7B-Instruct-v0.1\")\n",
        "\n",
        "# def Breeze_st(messages):\n",
        "#     prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "#     payload = json.dumps({\n",
        "#         \"inputs\": prompt,\n",
        "#         \"parameters\": {\n",
        "#             \"do_sample\": True,\n",
        "#             \"temperature\": 0.01,\n",
        "#             \"top_p\": 0.95\n",
        "#         }\n",
        "#     })\n",
        "#     headers = {\n",
        "#         'Content-Type': 'application/json',\n",
        "#         'accept': 'application/json'\n",
        "#     }\n",
        "\n",
        "#     response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "#     return_text = response.text\n",
        "\n",
        "#     return return_text\n",
        "\n",
        "# def translate_ch2amis(sentence, datastore_embeddings, knn_k=10, cot_num=2, findwords=True):\n",
        "#     # Get knn examples\n",
        "#     examples = find_knn_examples(sentence, datastore_embeddings, knn_k, findwords)\n",
        "#     cot_examples = find_knn_examples_topN_sentence(sentence, datastore_embeddings, cot_num)\n",
        "\n",
        "#     messages = []\n",
        "#     for i in cot_examples:\n",
        "#         messages.append({\"role\": \"user\", \"content\":\"You are an amis language translator. The followings some [zh] to [amis] examples. \\n\" + find_knn_examples(i[0], datastore_embeddings, knn_k, findwords) + \"If you see [*zh], it means that I couldn't find it in my dictionary, so I have identified the term closest in meaning. Please help me determine whether it has the same meaning as the next [zh] (the one I identified as the closest). If it does, please refer to it , If they are not similar, ignore it. If you see English, it means a proper noun, Please ignore what it brings back , and please put it directly into the translated sentence when translating, don't ignore it. cloud you help to translate[zh]:\" + i[0]})\n",
        "#         messages.append({\"role\": \"assistant\", \"content\": i[1]})\n",
        "\n",
        "#     messages.append({\"role\": \"user\", \"content\":\"You are an amis language translator. The followings some [zh] to [amis] examples. \\n\" + examples + \"If you see [*zh], it means that I couldn't find it in my dictionary, so I have identified the term closest in meaning. Please help me determine whether it has the same meaning as the next [zh] (the one I identified as the closest). If it does, please refer to it , If they are not similar, ignore it. cloud you help to translate[zh]:\" + sentence})\n",
        "\n",
        "#     return Breeze_st(messages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q86N3puDelMG"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eatk6-0jen1B"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def GPT_st(messages):\n",
        "  # try:\n",
        "    # Get response from openai\n",
        "    response = openai.ChatCompletion.create(\n",
        "      # model=\"gpt-4-1106-preview\",\n",
        "      model=\"gpt-3.5-turbo-0301\",\n",
        "      messages = messages,\n",
        "      max_tokens = 512,\n",
        "      temperature = 0\n",
        "    )['choices'][0]['message']['content']\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ZI2WLk6-kI"
      },
      "outputs": [],
      "source": [
        "def translate_ch2amis(sentence, datastore_embeddings, knn_k=10, cot_num=2, findwords=True):\n",
        "  # Get knn examples\n",
        "  examples = find_knn_examples(sentence, datastore_embeddings, knn_k, findwords)\n",
        "  cot_examples = find_knn_examples_topN_sentence(sentence, datastore_embeddings, cot_num)\n",
        "\n",
        "  messages = []\n",
        "  for i in cot_examples:\n",
        "    messages.append({\"role\": \"user\", \"content\":\"You are an amis language translator. The followings some [zh] to [amis] examples. \\n\" + find_knn_examples(i[0], datastore_embeddings, knn_k, findwords) + \"If you see [*zh], it means that I couldn't find it in my dictionary, so I have identified the term closest in meaning. Please help me determine whether it has the same meaning as the next [zh] (the one I identified as the closest). If it does, please refer to it , If they are not similar, ignore it. If you see English, it means a proper noun, Please ignore what it brings back , and please put it directly into the translated sentence when translating, don't ignore it. cloud you help to translate[zh]:\" + i[0]})\n",
        "    messages.append({\"role\": \"assistant\", \"content\": i[1]})\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\":\"You are an amis language translator. The followings some [zh] to [amis] examples. \\n\" + examples + \"If you see [*zh], it means that I couldn't find it in my dictionary, so I have identified the term closest in meaning. Please help me determine whether it has the same meaning as the next [zh] (the one I identified as the closest). If it does, please refer to it , If they are not similar, ignore it. cloud you help to translate[zh]:\" + sentence})\n",
        "\n",
        "  # print(examples)\n",
        "  return GPT_st(messages)\n",
        "\n",
        "# translate_ch2amis(\"我是Sawmah。\", all_datastore_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tVMVa_reSxl",
        "outputId": "7002bec2-e085-403d-d58f-087d89cf60cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "380\n"
          ]
        }
      ],
      "source": [
        "print(len(all_datastore_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpIoySkMCgp1",
        "outputId": "0dfae15a-58f1-417c-e095-4e59ffb27a21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(all_datastore_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIL9lHb_mLcg"
      },
      "source": [
        "### Batch translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny3jKwNYmOsx"
      },
      "outputs": [],
      "source": [
        "# 將原始中文資料翻譯成南勢阿美族語\n",
        "def translate_to_amei(chinese_data, datastore_embeddings, knn_k=10, cot_num=2, findwords=True):\n",
        "  result = {}\n",
        "  cnt = 1\n",
        "  for sentence in chinese_data:\n",
        "    print(str(cnt)+\".\")\n",
        "    print(sentence)\n",
        "    response = translate_ch2amis(sentence, datastore_embeddings, knn_k, cot_num, findwords)\n",
        "    # translated_sentence = response[response.find(\"[amis]:\")+7:].strip()\n",
        "\n",
        "    translated_sentence = response\n",
        "    print(translated_sentence + '\\n')\n",
        "    result[sentence] = translated_sentence\n",
        "    cnt += 1\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ZQVH6_Ve33"
      },
      "source": [
        "## iv. Learn from mistakes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F3TmRVVSHBj"
      },
      "source": [
        "### Create data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIr5b2VWk54r"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def create_LFM_data(mode, t_data):\n",
        "  if mode == 0:\n",
        "    data = t_data\n",
        "  else:\n",
        "    data = {}\n",
        "    for key,value in t_data.items():\n",
        "      temp = key.split()\n",
        "      random.shuffle(temp)\n",
        "      data[value] = \" \".join(temp)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtXb2_iokzqs"
      },
      "source": [
        "### Find similar in knn and translate it (as wrong answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBYTmxQ3Ll2q"
      },
      "outputs": [],
      "source": [
        "def create_wrong_set(mode, topN_sentence, datastore_embeddings):\n",
        "  wr = []\n",
        "\n",
        "  if mode==0:\n",
        "    for i in topN_sentence:\n",
        "      #翻譯 (knn func 已排除找到原句的狀況)\n",
        "      response = translate_ch2amis(i[0], datastore_embeddings)\n",
        "      wr.append([i[0], response, i[1]])\n",
        "  else:\n",
        "    for i in topN_sentence:\n",
        "      temp = i[1].split()\n",
        "      random.shuffle(temp)\n",
        "      response = \" \".join(temp)\n",
        "      # print(response)\n",
        "      wr.append([i[0], response, i[1]])\n",
        "\n",
        "  return wr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNe3nBZHoqrd"
      },
      "outputs": [],
      "source": [
        "def find_wrong_example(mode, sentence, datastore_embeddings, k=2):\n",
        "\n",
        "  # 找到與目標sentence最相似的k句話(預設k=2)\n",
        "  topN_sentence = find_knn_examples_topN_sentence(sentence, datastore_embeddings, k)\n",
        "  # print(topN_sentence)\n",
        "\n",
        "  return create_wrong_set(mode, topN_sentence, datastore_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k7HR6L2WiEA"
      },
      "source": [
        "### Error checking func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2BeiHA8f3Cw"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import json\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# url = \"\"\n",
        "\n",
        "# #tokenizer = AutoTokenizer.from_pretrained(\"MediaTek-Research/Breeze-7B-Instruct-v0.1\")\n",
        "\n",
        "# def Breeze_error_checking(messages, cnt):\n",
        "#     prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "#     payload = json.dumps({\n",
        "#         \"inputs\": prompt,\n",
        "#         \"parameters\": {\n",
        "#             \"do_sample\": True,\n",
        "#             \"temperature\": 0.01,\n",
        "#             \"top_p\": 0.95\n",
        "#         }\n",
        "#     })\n",
        "#     headers = {\n",
        "#         'Content-Type': 'application/json',\n",
        "#         'accept': 'application/json'\n",
        "#     }\n",
        "\n",
        "#     response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "#     return_text = response.text\n",
        "\n",
        "#     if return_text.find('\\n') != -1 and cnt < 10:\n",
        "#         print(\"[re-generating...]\")\n",
        "#         cnt += 1\n",
        "#         return Breeze_error_checking(messages, cnt)\n",
        "#     else:\n",
        "#         return return_text\n",
        "\n",
        "# def error_checking(sentence, amis, wrong_set, pf, datastore_embeddings, k, findwords):\n",
        "#     hint = \"\"\n",
        "#     for i in wrong_set:\n",
        "#         if not hint:\n",
        "#             hint += \"[zh]:\" + i[0] + \" [your answer]:\" + i[1] + \" [correct answer]:\" + i[2]\n",
        "#         else:\n",
        "#             hint += \" ; [zh]:\" + i[0] + \" [your answer]:\" + i[1] + \" [correct answer]:\" + i[2]\n",
        "\n",
        "#     examples = find_knn_examples(sentence, datastore_embeddings, k, findwords)\n",
        "\n",
        "#     messages = []\n",
        "#     for i in pf:\n",
        "#         messages.append({\"role\": \"user\", \"content\": \"Confirm whether the following sentence needs revision: [zh]:\"+ i[0] + \" [your answer]:\" + i[1] + \" \\n\" + \"Just answer me the [Revision]. Just answer me the [Revision]. Just answer me the [Revision].\"})\n",
        "#         messages.append({\"role\": \"assistant\", \"content\": i[2]})\n",
        "\n",
        "#     messages.append({\"role\": \"system\", \"content\": \"Here are several sets of results you translated before: \" + hint + \" \\n\" + \\\n",
        "#               \"Please analyze the differences between [your answer] and [Revision] result in contexts. \" + \\\n",
        "#               \"Mainly focusing on the arrangement of words in sentences, learning their structure and grammar. Just change their order. Just change their order. Just change their order.\\n\"})\n",
        "\n",
        "#     messages.append({\"role\": \"user\", \"content\": \"These are some tips of generating your case: \" + examples + \"\\n\" + \\\n",
        "#             \"Confirm whether the following sentence needs revision: [zh]:\"+ sentence + \"[your answer]:\" + amis + \"\\n\" + \\\n",
        "#             \"Just answer me the [Revision]. Just answer me the [Revision]. Just answer me the [Revision].\"})\n",
        "\n",
        "#     cnt = 0\n",
        "#     response = Breeze_error_checking(messages, cnt)\n",
        "#     print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqCLLgufTqGV"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import json\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# url = \"\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"MediaTek-Research/Breeze-7B-Instruct-v0.1\")\n",
        "\n",
        "# def Breeze_error_checking(messages, cnt):\n",
        "#     prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "#     payload = json.dumps({\n",
        "#         \"inputs\": prompt,\n",
        "#         \"parameters\": {\n",
        "#             \"do_sample\": True,\n",
        "#             \"temperature\": 0.01,\n",
        "#             \"top_p\": 0.95\n",
        "#         }\n",
        "#     })\n",
        "#     headers = {\n",
        "#         'Content-Type': 'application/json',\n",
        "#         'accept': 'application/json'\n",
        "#     }\n",
        "\n",
        "#     response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "#     return_text = response.text\n",
        "\n",
        "#     if return_text.find('\\n') != -1 and cnt < 10:\n",
        "#         print(\"[re-generating...]\")\n",
        "#         cnt += 1\n",
        "#         return Breeze_error_checking(messages, cnt)\n",
        "#     else:\n",
        "#         return return_text\n",
        "\n",
        "# def error_checking(sentence, amis, wrong_set, pf, datastore_embeddings, k, findwords):\n",
        "#     hint = \"\"\n",
        "#     for i in wrong_set:\n",
        "#         if not hint:\n",
        "#             hint += \"[zh]:\" + i[0] + \" [your answer]:\" + i[1] + \" [correct answer]:\" + i[2]\n",
        "#         else:\n",
        "#             hint += \" ; [zh]:\" + i[0] + \" [your answer]:\" + i[1] + \" [correct answer]:\" + i[2]\n",
        "\n",
        "#     examples = find_knn_examples(sentence, datastore_embeddings, k, findwords)\n",
        "\n",
        "#     messages = []\n",
        "#     for i in pf:\n",
        "#         messages.append({\"role\": \"user\", \"content\": \"Confirm whether the following sentence needs revision: [zh]:\"+ i[0] + \" [your answer]:\" + i[1] + \" \\n\" + \"Just answer me the [Revision]. Just answer me the [Revision]. Just answer me the [Revision].\"})\n",
        "#         messages.append({\"role\": \"assistant\", \"content\": i[2]})\n",
        "\n",
        "#     messages.append({\"role\": \"system\", \"content\": \"Here are several sets of results you translated before: \" + hint + \" \\n\" + \\\n",
        "#               \"Please analyze the differences between [your answer] and [Revision] result in contexts. \" + \\\n",
        "#               \"Mainly focusing on the arrangement of words in sentences, learning their structure and grammar. Just change their order. Just change their order. Just change their order.\\n\"})\n",
        "\n",
        "#     messages.append({\"role\": \"user\", \"content\": \"These are some tips of generating your case: \" + examples + \"\\n\" + \\\n",
        "#             \"Confirm whether the following sentence needs revision: [zh]:\"+ sentence + \"[your answer]:\" + amis + \"\\n\" + \\\n",
        "#             \"Just answer me the [Revision]. Just answer me the [Revision]. Just answer me the [Revision].\"})\n",
        "\n",
        "#     cnt = 0\n",
        "#     return Breeze_error_checking(messages, cnt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY4_QjgWmQyE"
      },
      "outputs": [],
      "source": [
        "# def GPT_error_checking(messages, cnt):\n",
        "#   try:\n",
        "#     response = openai.ChatCompletion.create(\n",
        "#     # model=\"gpt-3.5-turbo-16k-0613\",\n",
        "#     model=\"gpt-4-1106-preview\",\n",
        "#     messages = messages,\n",
        "#     max_tokens = 512,\n",
        "#     temperature = 0\n",
        "#     )['choices'][0]['message']['content']\n",
        "#     if response.find('\\n') != -1 and cnt < 10:\n",
        "#       #生成不只一行的話，代表除了Revision外，還生了其他東西\n",
        "#       #給他10次機會簡單的過濾，不然就直接輸出\n",
        "#       #當然只有一行也可能有其他東西，生完還是要人工審核跟改正一下\n",
        "#       print(\"[re-generating...]\")\n",
        "#       cnt += 1\n",
        "#       return GPT_error_checking(messages, cnt)\n",
        "#     else:\n",
        "#       return response\n",
        "#   except:\n",
        "#      time.sleep(0.5)\n",
        "#      return GPT_error_checking(messages, cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVf6Hmu4mQyK"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "# def error_checking(sentence, amis, wrong_set, pf, datastore_embeddings, k, findwords):\n",
        "\n",
        "#   hint = \"\"\n",
        "#   for i in wrong_set:\n",
        "#     if not hint:\n",
        "#       hint += \"[zh]:\" + i[0] + \" [your answer]:\" + i[1] + \" [correct answer]:\" + i[2]\n",
        "#     else:\n",
        "#       hint += \" ; [zh]:\" + i[0] + \" [your answer]:\" + i[1] + \" [correct answer]:\" + i[2]\n",
        "#   # print(hint)\n",
        "\n",
        "#   examples = find_knn_examples(sentence, datastore_embeddings, k, findwords)\n",
        "\n",
        "#   messages = []\n",
        "#   for i in pf:\n",
        "#     messages.append({\"role\": \"user\", \"content\": \"Confirm whether the following sentence needs revision: [zh]:\"+ i[0] + \" [your answer]:\" + i[1] + \" \\n\" + \"Just answer me the [Revision]. Just answer me the [Revision]. Just answer me the [Revision].\"})\n",
        "#     messages.append({\"role\": \"assistant\", \"content\": i[2]})\n",
        "\n",
        "#   messages.append({\"role\": \"system\", \"content\": \"Here are several sets of results you translated before: \" + hint + \" \\n\" + \\\n",
        "#               \"Please analyze the differences between [your answer] and [Revision] result in contexts. \" + \\\n",
        "#               \"Mainly focusing on the arrangement of words in sentences, learning their structure and grammar. Just change their order. Just change their order. Just change their order.\\n\"})\n",
        "\n",
        "#   messages.append({\"role\": \"user\", \"content\": \"These are some tips of generating your case: \" + examples + \"\\n\" + \\\n",
        "#             \"Confirm whether the following sentence needs revision: [zh]:\"+ sentence + \"[your answer]:\" + amis + \"\\n\" + \\\n",
        "#             \"Just answer me the [Revision]. Just answer me the [Revision]. Just answer me the [Revision].\"})\n",
        "\n",
        "#   cnt = 0\n",
        "#   return GPT_error_checking(messages, cnt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxpbuGBgWnYc"
      },
      "source": [
        "### Batch error checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PFcdmyHkgp1"
      },
      "outputs": [],
      "source": [
        "def batch_error_checking(mode, translated_data, pf, datastore_embeddings, lfm_k, knn_k, findwords=True):\n",
        "    result = {}\n",
        "    cnt = 1\n",
        "    for sentence in translated_data.keys():\n",
        "      print(str(cnt)+\".\")\n",
        "      print(sentence)\n",
        "      wrong_set = find_wrong_example(mode, sentence, datastore_embeddings, lfm_k)\n",
        "      print(wrong_set)\n",
        "      response = translated_data[sentence]\n",
        "      print(response)\n",
        "      revision = error_checking(sentence, response, wrong_set, pf, datastore_embeddings, knn_k, findwords)\n",
        "\n",
        "      translated_sentence = revision\n",
        "\n",
        "      print(translated_sentence + \"\\n\")\n",
        "      result[sentence] = translated_sentence\n",
        "      cnt += 1\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSSJ3vE8m80O"
      },
      "source": [
        "## v. Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNANIgSYnB07"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', punctuation))\n",
        "\n",
        "def replace_quotes(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        for key, value in obj.items():\n",
        "            obj[key] = replace_quotes(value)\n",
        "    elif isinstance(obj, list):\n",
        "        for i in range(len(obj)):\n",
        "            obj[i] = replace_quotes(obj[i])\n",
        "    elif isinstance(obj, str):\n",
        "        obj = obj.replace(\"'\", \"’\")\n",
        "    return obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60UyPuEUnITD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from string import punctuation\n",
        "\n",
        "def bleu_scoring(translated_data, chinese_data, scoring_result=0):\n",
        "  # bleu_scores = []\n",
        "  bleu_scores_results = []  # List to store the results for saving in JSON format\n",
        "\n",
        "\n",
        "  bleu_scores = []\n",
        "  combined_data = []\n",
        "  for ref, hyp in zip(chinese_data, translated_data.values()):\n",
        "    hyp = replace_quotes(hyp)\n",
        "    print(hyp)\n",
        "\n",
        "    reference = [remove_punctuation(ref).lower().split()]  # Remove punctuation and convert to lowercase\n",
        "    hypothesis = remove_punctuation(hyp).lower().split()   # Remove punctuation and convert to lowercase\n",
        "\n",
        "    # print(reference)\n",
        "    # print(hypothesis)\n",
        "\n",
        "    bleu_score = sentence_bleu(reference, hypothesis)\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "    # Append the results to the list\n",
        "    if not scoring_result:\n",
        "      combined_data.append({\n",
        "          \"參考句子\": ref,\n",
        "          \"翻譯句子\": hyp,\n",
        "          \"BLEU-1\": sentence_bleu(reference, hypothesis, weights=(1, 0, 0, 0)),\n",
        "          \"BLEU-2\": sentence_bleu(reference, hypothesis, weights=(0.5, 0.5, 0, 0)),\n",
        "          \"BLEU-3\": sentence_bleu(reference, hypothesis, weights=(0.33, 0.33, 0.33, 0)),\n",
        "          \"BLEU-4\": sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25)),\n",
        "          \"BLEU 分數\": bleu_score\n",
        "      })\n",
        "    else:\n",
        "      combined_data.append({\n",
        "          \"參考句子\": ref,\n",
        "          \"翻譯句子\": scoring_result[len(combined_data)][\"翻譯句子\"],\n",
        "          \"錯誤反思\": hyp,\n",
        "          \"BLEU-1(原,LFM)\": [scoring_result[len(combined_data)][\"BLEU-1\"], sentence_bleu(reference, hypothesis, weights=(1, 0, 0, 0))],\n",
        "          \"BLEU-2(原,LFM)\": [scoring_result[len(combined_data)][\"BLEU-2\"], sentence_bleu(reference, hypothesis, weights=(0.5, 0.5, 0, 0))],\n",
        "          \"BLEU-3(原,LFM)\": [scoring_result[len(combined_data)][\"BLEU-3\"], sentence_bleu(reference, hypothesis, weights=(0.33, 0.33, 0.33, 0))],\n",
        "          \"BLEU-4(原,LFM)\": [scoring_result[len(combined_data)][\"BLEU-4\"], sentence_bleu(reference, hypothesis, weights=(0.25, 0.25, 0.25, 0.25))],\n",
        "          \"BLEU 分數\": bleu_score\n",
        "      })\n",
        "\n",
        "  # Calculate the average BLEU scores\n",
        "  total_bleu_1 = 0\n",
        "  total_bleu_2 = 0\n",
        "  total_bleu_3 = 0\n",
        "  total_bleu_4 = 0\n",
        "  count = len(combined_data)\n",
        "  print(count)\n",
        "  for entry in combined_data:\n",
        "    if not scoring_result:\n",
        "      total_bleu_1 += entry.get('BLEU-1', 0)\n",
        "      total_bleu_2 += entry.get('BLEU-2', 0)\n",
        "      total_bleu_3 += entry.get('BLEU-3', 0)\n",
        "      total_bleu_4 += entry.get('BLEU-4', 0)\n",
        "    else:\n",
        "      total_bleu_1 += entry['BLEU-1(原,LFM)'][1]\n",
        "      total_bleu_2 += entry['BLEU-2(原,LFM)'][1]\n",
        "      total_bleu_3 += entry['BLEU-3(原,LFM)'][1]\n",
        "      total_bleu_4 += entry['BLEU-4(原,LFM)'][1]\n",
        "\n",
        "  avg_bleu_1 = total_bleu_1 / count\n",
        "  avg_bleu_2 = total_bleu_2 / count\n",
        "  avg_bleu_3 = total_bleu_3 / count\n",
        "  avg_bleu_4 = total_bleu_4 / count\n",
        "\n",
        "  # Add the average BLEU scores as a new entry\n",
        "  if not scoring_result:\n",
        "    avg_entry = {\n",
        "        \"Average BLEU-1\": avg_bleu_1,\n",
        "        \"Average BLEU-2\": avg_bleu_2,\n",
        "        \"Average BLEU-3\": avg_bleu_3,\n",
        "        \"Average BLEU-4\": avg_bleu_4\n",
        "    }\n",
        "  else:\n",
        "    avg_entry = {\n",
        "        \"Average BLEU-1(原,LFM)\": [scoring_result[len(combined_data)+1][\"Average BLEU-1\"], avg_bleu_1],\n",
        "        \"Average BLEU-2(原,LFM)\": [scoring_result[len(combined_data)+1][\"Average BLEU-2\"], avg_bleu_2],\n",
        "        \"Average BLEU-3(原,LFM)\": [scoring_result[len(combined_data)+1][\"Average BLEU-3\"], avg_bleu_3],\n",
        "        \"Average BLEU-4(原,LFM)\": [scoring_result[len(combined_data)+1][\"Average BLEU-4\"], avg_bleu_4]\n",
        "    }\n",
        "\n",
        "  print(\"Average BLEU-1:\", avg_bleu_1)\n",
        "  print(\"Average BLEU-2:\", avg_bleu_2)\n",
        "  print(\"Average BLEU-3:\", avg_bleu_3)\n",
        "  print(\"Average BLEU-4:\", avg_bleu_4)\n",
        "\n",
        "\n",
        "  # Calculate and print the average BLEU score\n",
        "  avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "  combined_data.append({\"平均 BLEU 分數\": avg_bleu_score})\n",
        "\n",
        "  combined_data.append(avg_entry)\n",
        "\n",
        "  return combined_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpMwkrD9fxU8"
      },
      "source": [
        "# 2. Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPx2_x3jf79a"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-bzEtzUgADs"
      },
      "outputs": [],
      "source": [
        "# Modes\n",
        "Language = \"海岸阿美族語\"\n",
        "\n",
        "Random_select = True # 是否要從Datastore中隨機抽取100筆資料 (如果False，要提供 Test_data_path)\n",
        "\n",
        "Download_random_data = True # No effect if Random_select is False\n",
        "\n",
        "Test_data_path = \"\" # No effect if Random_select is True (Only effect when Random_select is False)\n",
        "\n",
        "#---------------------------------------\n",
        "# Knn parameters\n",
        "Knn_k = 10\n",
        "COT_num = 2\n",
        "Find_words = True #是否要提供詞表作為參考\n",
        "file_title = \"(海岸阿美_k=10_COT_2e)\"\n",
        "\n",
        "#---------------------------------------\n",
        "# Learn from mistakes(or re-ordering) parameters\n",
        "\n",
        "LFM_test_data_mode = 0 # 0: Data from \"Translating Pipeline\" 1: Data from \"Re-ordering Correct Answer\" (Mode 1 only use for testing and observation)\n",
        "\n",
        "LFM_ref_data_mode = 0 # Reference from \"Translating Mistakes\" 1: Reference from \"Re-ordering Correct Answer\"\n",
        "\n",
        "LFM_k = 2\n",
        "LFM_find_words = True #是否要提供詞表作為參考\n",
        "LFM_file_title = \"(海岸阿美_Shuffle_LFM)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_jY4O0EvsyA"
      },
      "source": [
        "###### main functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHscfnkB4NNl"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "def save_and_download(info, data, path):\n",
        "  with open(path, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=2)\n",
        "  files.download(path)\n",
        "  print(f\"已將{info}存至: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haY2EFz-vyfN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "def create_test_data(random_select, download_random_data, Test_data_path, version):\n",
        "\n",
        "  test_datastore_embeddings = dict(all_datastore_embeddings)\n",
        "\n",
        "  if random_select == False:\n",
        "    test_data = json.load(open(Test_data_path, \"r\"))\n",
        "    for i in test_data.values():\n",
        "      del test_datastore_embeddings[i]\n",
        "  else:\n",
        "    Test_data_path = \"\"\n",
        "    test_data = {}\n",
        "    for i in range(100):\n",
        "      temp = random.choice(list(test_datastore_embeddings.keys()))\n",
        "      test_data[all_ch2amis[temp]] = temp\n",
        "      del test_datastore_embeddings[temp]\n",
        "\n",
        "    if Download_random_data == True:\n",
        "      Test_data_path = \"/content/\" + Language + \"_隨機抽取100筆_v\" + str(version) + \".json\"\n",
        "      save_and_download(\"test_data\", test_data, Test_data_path)\n",
        "  return test_data, test_datastore_embeddings, Test_data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9H_pNCzSlko"
      },
      "outputs": [],
      "source": [
        "def get_incexp(k):\n",
        "  pf = []\n",
        "  for i in range(k):\n",
        "    while(True):\n",
        "      temp = random.choice(list(all_ch2amis.items()))\n",
        "      if temp[1] not in test_data.keys():\n",
        "        break\n",
        "    pf.append(temp)\n",
        "\n",
        "  return create_wrong_set(LFM_ref_data_mode, pf, test_datastore_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLDfE7h4K3SZ"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "4Uq31nH-LHTG",
        "outputId": "dfe75c2f-de30-4f92-a197-8f8495f10383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running version: 1\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c507b70a-20c9-4790-b89a-8eeec59cc2aa\", \"\\u6d77\\u5cb8\\u963f\\u7f8e\\u65cf\\u8a9e_\\u96a8\\u6a5f\\u62bd\\u53d6100\\u7b46_v1.json\", 8392)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已將test_data存至: /content/海岸阿美族語_隨機抽取100筆_v1.json\n"
          ]
        }
      ],
      "source": [
        "version = 1\n",
        "print(\"Running version:\", version)\n",
        "\n",
        "#建立測試資料集 (test_data: 100句正確答案， test_datastore_embeddings: 其餘約350句)\n",
        "test_data, test_datastore_embeddings, Test_data_path = create_test_data(Random_select, Download_random_data, Test_data_path, version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18cUwOSY_uCV",
        "outputId": "7a247f50-7210-4d6d-9852-c49ad6cafbc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.28.1\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "print(openai.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvvJ0QEdh43a"
      },
      "source": [
        "##### Translate pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "EWKx978OiCCw",
        "outputId": "2b3fe18c-bc42-47ee-8b13-9de8b7e81c63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.\n",
            "沒事我還好，謝謝老師。\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-ce5944382c5f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mLFM_test_data_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# 將中文翻譯成原住民語\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtranslated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_to_amei\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_datastore_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKnn_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOT_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFind_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# 儲存翻譯檔案\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-c141c2ebcf12>\u001b[0m in \u001b[0;36mtranslate_to_amei\u001b[0;34m(chinese_data, datastore_embeddings, knn_k, cot_num, findwords)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_ch2amis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatastore_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcot_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfindwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# translated_sentence = response[response.find(\"[amis]:\")+7:].strip()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtranslated_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d52c32296bb6>\u001b[0m in \u001b[0;36mtranslate_ch2amis\u001b[0;34m(sentence, datastore_embeddings, knn_k, cot_num, findwords)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_ch2amis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatastore_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcot_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfindwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_knn_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatastore_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfindwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mcot_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_knn_examples_topN_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatastore_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcot_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-07d2a457ac3e>\u001b[0m in \u001b[0;36mfind_knn_examples\u001b[0;34m(sentence, datastore_embeddings, k, findwords)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfind_knn_examples_topN_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatastore_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mexamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"[zh]:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mexamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"[amis]:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-9dcb875feb35>\u001b[0m in \u001b[0;36mfind_knn_examples_topN_sentence\u001b[0;34m(sentence, datastore_embeddings, k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatastore_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_single_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f26b2534eb73>\u001b[0m in \u001b[0;36mget_single_embedding\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 把句子轉換為嵌入向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_single_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# input type: string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2235\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "if not LFM_test_data_mode:\n",
        "  # 將中文翻譯成原住民語\n",
        "  translated_data = translate_to_amei(list(test_data.values()), test_datastore_embeddings, Knn_k, COT_num, Find_words)\n",
        "\n",
        "  # 儲存翻譯檔案\n",
        "  save_and_download(\"翻譯結果\", translated_data, \"/content/\" + file_title + \"translated_data_v\" + str(version) + \".json\")\n",
        "\n",
        "  # Scoring\n",
        "  scoring_result = bleu_scoring(translated_data, test_data)\n",
        "  save_and_download(\"評分結果\", scoring_result, \"/content/\" + file_title + \"BleuScore_v\" + str(version) + \".json\")\n",
        "\n",
        "else:\n",
        "  translated_data = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHvs7m6cJsYF"
      },
      "source": [
        "##### Learn from mistakes pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "D8XN6ZvLKB4x",
        "outputId": "1c8f377d-3747-4c6c-c824-74a7047192b7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'translated_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-785a2cc77667>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 建立或讀取測試資料集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mLFM_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_LFM_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLFM_test_data_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtranslated_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLFM_test_data_mode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LFM_test_data_mode=1 時，先對資料集評分\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mLFM_test_data_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'translated_data' is not defined"
          ]
        }
      ],
      "source": [
        "# 建立或讀取測試資料集\n",
        "LFM_test_data = create_LFM_data(LFM_test_data_mode, list([translated_data, test_data])[LFM_test_data_mode])\n",
        "\n",
        "# LFM_test_data_mode=1 時，先對資料集評分\n",
        "if LFM_test_data_mode:\n",
        "  scoring_result = bleu_scoring(LFM_test_data, test_data)\n",
        "  save_and_download(\"打亂結果\", scoring_result, \"/content/Shuffle_BleuScore_v\" + str(version) + \".json\")\n",
        "\n",
        "# 執行 Revision\n",
        "pf = get_incexp(2)\n",
        "LFM_result = batch_error_checking(LFM_ref_data_mode, LFM_test_data, pf, test_datastore_embeddings, LFM_k, Knn_k, LFM_find_words)\n",
        "\n",
        "# 儲存Revision檔案\n",
        "save_and_download(\"Revision結果\", LFM_result, \"/content/\" + LFM_file_title + \"revision_data_v\" + str(version) + \"_gpt4.json\")\n",
        "\n",
        "# Scoring\n",
        "LFM_scoring_result = bleu_scoring(LFM_result, test_data, scoring_result)\n",
        "save_and_download(\"LFM評分結果\", LFM_scoring_result, \"/content/\" + LFM_file_title + \"BleuScore_v\" + str(version) + \"_gpt4.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpC8QS91rvNf"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCKEeR8HTH2Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
